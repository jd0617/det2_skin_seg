{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a4b33b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from detectron2.structures import Boxes, BoxMode\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self.hook_handles = []\n",
    "\n",
    "        # register hooks\n",
    "        self.hook_handles.append(target_layer.register_forward_hook(self._save_activation))\n",
    "        self.hook_handles.append(target_layer.register_backward_hook(self._save_gradient))\n",
    "\n",
    "    def _save_activation(self, module, input, output):\n",
    "        if output.ndim == 3:\n",
    "            output = output.unsqueeze(0)\n",
    "        self.activations = output.detach()\n",
    "\n",
    "    def _save_gradient(self, module, grad_input, grad_output):\n",
    "        grad = grad_output[0]\n",
    "        if grad.ndim == 3:\n",
    "            grad = grad.unsqueeze(0)\n",
    "        self.gradients = grad.detach()\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        for h in self.hook_handles:\n",
    "            h.remove()\n",
    "\n",
    "    def __call__(self, input_tensor, class_idx=None):\n",
    "        # Forward\n",
    "        output = self.model(input_tensor)\n",
    "        \n",
    "        # If it's a detection model, you'll need to pick a score or logit\n",
    "        if class_idx is None:\n",
    "            score = output[0][\"instances\"].scores[0]  # first detection\n",
    "        else:\n",
    "            # print(\"class_idx: \", class_idx)\n",
    "            # print(output[0])\n",
    "            score = output[0][\"instances\"].scores[int(class_idx)]\n",
    "\n",
    "        # Backward\n",
    "        self.model.zero_grad()\n",
    "        score.backward(retain_graph=True)\n",
    "\n",
    "        # Global-average-pool gradients\n",
    "        weights = self.gradients.mean(dim=[2, 3], keepdim=True)  # [C,1,1]\n",
    "\n",
    "        # Weighted sum of activations\n",
    "        cam = (weights * self.activations).sum(dim=1, keepdim=True)\n",
    "        cam = torch.relu(cam)\n",
    "        \n",
    "        # Normalize to [0,1]\n",
    "        cam = cam.squeeze().cpu().numpy()\n",
    "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
    "        return cam\n",
    "\n",
    "\n",
    "# ---- Visualizer helper ----\n",
    "def draw_predictions(img, outputs, metadata, gt_boxes, gt_classes):\n",
    "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
    "    inst = outputs[\"instances\"].to(\"cpu\")\n",
    "    # v = v.draw_instance_predictions(inst)\n",
    "\n",
    "    # if gt_boxes is not None:\n",
    "        # Draw GT boxes in red\n",
    "    for box, cls in zip(gt_boxes, gt_classes):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        cls_name = metadata.thing_classes[cls] if len(metadata.thing_classes) > 0 else str(cls)\n",
    "        text = f\"GT: {cls_name}\"\n",
    "\n",
    "        v.draw_box([x1, y1, x2, y2], edge_color=(0.0,1.0,0.0))\n",
    "        v.draw_text(text, (x1, y1), color=(0.0,1.0,0.0))\n",
    "\n",
    "    boxes = inst.pred_boxes.tensor.numpy()\n",
    "    scores = inst.scores.tolist()\n",
    "    classes = inst.pred_classes.tolist()\n",
    "\n",
    "    for box, score, cls_id in zip(boxes, scores, classes):\n",
    "        x0, y0, x1, y1 = box\n",
    "        cls_name = metadata.thing_classes[cls_id] if len(metadata.thing_classes) > 0 else str(cls_id)\n",
    "        text = f\"Pred: {cls_name} {score:.2f}\"\n",
    "\n",
    "        v.draw_box([x0, y0, x1, y1], edge_color=(1.0, 0.0, 0.0))\n",
    "        v.draw_text(text, (x0, y0), color=(1.0, 0.0, 0.0))\n",
    "\n",
    "    vis_out = v.output\n",
    "    drawn = vis_out.get_image()[:, :, ::-1]\n",
    "\n",
    "    return drawn\n",
    "\n",
    "\n",
    "def get_gt_from_dict(entry):\n",
    "    gt_boxes, gt_classes = [], []\n",
    "    for ann in entry[\"annotations\"]:\n",
    "        bbox = ann[\"bbox\"]\n",
    "        if ann[\"bbox_mode\"] != BoxMode.XYXY_ABS:\n",
    "            bbox = BoxMode.convert(bbox, ann[\"bbox_mode\"], BoxMode.XYXY_ABS)\n",
    "        gt_boxes.append(bbox)\n",
    "        gt_classes.append(ann[\"category_id\"])\n",
    "    return gt_boxes, gt_classes  \n",
    "\n",
    "\n",
    "# ---- Main routine for one image ----\n",
    "def visualize_cam_and_bboxes(entry, model, gradcam, metadata, out_dir):\n",
    "    img_path = entry[\"file_name\"]\n",
    "    h, w = entry[\"height\"], entry[\"width\"]\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    # Detectron2 input\n",
    "    inputs = [{\"image\": torch.as_tensor(img.astype(\"float32\").transpose(2, 0, 1)).cuda(),\n",
    "               \"height\": h, \"width\": w}]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "\n",
    "    # Grad-CAM on top detection (if any)\n",
    "    if len(outputs[0][\"instances\"]) > 0:\n",
    "        score = outputs[0][\"instances\"].scores[0]\n",
    "        cam_map = gradcam(inputs, score)\n",
    "        cam_resized = cv2.resize(cam_map, (w, h))\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * cam_resized), cv2.COLORMAP_JET)\n",
    "        overlay = (0.5 * heatmap + 0.5 * img).astype(np.uint8)\n",
    "    else:\n",
    "        overlay = img.copy()\n",
    "\n",
    "    # GT boxes\n",
    "    gt_boxes, gt_classes = get_gt_from_dict(entry)\n",
    "\n",
    "    # print(\"GT boxes:\", gt_boxes[:3])\n",
    "    # print(\"GT classes:\", gt_classes[:3])\n",
    "\n",
    "    # Pred + GT\n",
    "    bbox_vis = draw_predictions(img.copy(), outputs[0], metadata, gt_boxes, gt_classes)\n",
    "\n",
    "    # Stack horizontally\n",
    "    stacked = np.hstack([overlay, bbox_vis])\n",
    "\n",
    "    # Save\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    out_path = Path(out_dir) / Path(img_path).name\n",
    "    cv2.imwrite(str(out_path), stacked)\n",
    "    print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "078cca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import _init_paths\n",
    "from config import cfg, update_config\n",
    "\n",
    "def update_cfg_with_args(cfg, arg_key, arg_value):\n",
    "    cfg.defrost()\n",
    "\n",
    "    arg_key = arg_key.upper()\n",
    "\n",
    "    cfg.arg_key = arg_value\n",
    "\n",
    "    cfg.freeze()\n",
    "\n",
    "\n",
    "def minmax_norm(img):\n",
    "    minmax_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    img = np.array([\n",
    "        minmax_scaler.fit_transform(img[:, :, 0]),\n",
    "        minmax_scaler.fit_transform(img[:, :, 1]),\n",
    "        minmax_scaler.fit_transform(img[:, :, 2]),\n",
    "    ])\n",
    "\n",
    "    return np.transpose(img, (1, 2, 0)).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68815a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.data import DatasetCatalog\n",
    "\n",
    "from dataset.utils import register_patch_bin_dataset\n",
    "\n",
    "\n",
    "cfg_path = \"/workspace/project/configs/frcnn/frcnn.yaml\"\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\n",
    "        \"COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml\"\n",
    "))\n",
    "\n",
    "cfg.set_new_allowed(True)\n",
    "cfg.defrost()\n",
    "cfg.merge_from_file(cfg_path)\n",
    "\n",
    "cfg.MODEL.WEIGHTS = \"/workspace/project/record/debug2/result_single/frcnn_vis/model_final.pth\"\n",
    "cfg.MODEL.DEVICE = \"cuda\"\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.8   # e.g., 0.5, Removes duplicate boxes that overlap too much\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.9 # detection score threshold, Removes weak predictions\n",
    "\n",
    "cfg.freeze()\n",
    "\n",
    "predictor = DefaultPredictor(cfg)\n",
    "model = predictor.model\n",
    "model.eval()\n",
    "\n",
    "target_layer = model.backbone.bottom_up.res5[-1]\n",
    "gradcam = GradCAM(model, target_layer)\n",
    "\n",
    "register_patch_bin_dataset(\n",
    "        cfg.DATASETS.TEST[0],\n",
    "        json_file=cfg.DATASETS.TEST_ANNO_DIR,\n",
    "        img_root=cfg.DATASETS.IMG_DIR,\n",
    "        extra_key=[\"patient_id\"]\n",
    ")\n",
    "\n",
    "ds_dicts = DatasetCatalog.get(cfg.DATASETS.TEST[0])\n",
    "metadata = MetadataCatalog.get(cfg.DATASETS.TEST[0])\n",
    "\n",
    "out_dir = \"./gradcam_results/gradcam_results_80nms_90score\"\n",
    "\n",
    "for i, entry in enumerate(ds_dicts):\n",
    "        visualize_cam_and_bboxes(entry, model, gradcam, metadata, out_dir)\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027e59c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "img_path = \"/workspace/datasets/seg_by_patient/preprocessed/pos_cropped_patch_all_r1_r2/raw1_01-D2(30m)-1.JPG\"\n",
    "# Prepare image\n",
    "img = cv2.imread(img_path)[:, :, ::-1]  # BGR->RGB\n",
    "\n",
    "img = minmax_norm(img)\n",
    "\n",
    "# inputs = predictor.transform_gen.get_transform(img).apply_image(img)\n",
    "tensor = torch.as_tensor(img.transpose(2,0,1)).cuda().float()\n",
    "\n",
    "input = [{\n",
    "    \"image\":tensor,\n",
    "    \"height\": 1200,\n",
    "    \"width\": 600\n",
    "}]\n",
    "\n",
    "# print(tensor.shape)\n",
    "\n",
    "# Generate CAM\n",
    "cam_map = gradcam(input)\n",
    "\n",
    "# Overlay heatmap\n",
    "heatmap = cv2.applyColorMap(np.uint8(255 * cam_map), cv2.COLORMAP_JET)\n",
    "\n",
    "H, W, _ = img.shape\n",
    "\n",
    "r_hm = cv2.resize(heatmap, (W, H))\n",
    "\n",
    "r_hm = (r_hm-r_hm.min()) / (r_hm.max()-r_hm.min()+1e-8)\n",
    "\n",
    "heatmap = cv2.applyColorMap(np.uint8(255 * r_hm), cv2.COLORMAP_JET)\n",
    "\n",
    "\n",
    "overlay = 0.5 * heatmap[:, :, ::-1] + 0.5 * img\n",
    "cv2.imwrite(\"./gradcam_custom.jpg\", overlay[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34e1146",
   "metadata": {},
   "source": [
    "# EigenCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90cf12d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "\n",
    "# -------- EigenCAM --------\n",
    "class EigenCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.activations = None\n",
    "        self.hook = self.target_layer.register_forward_hook(self._hook_fn)\n",
    "\n",
    "    def _hook_fn(self, module, inputs, output):\n",
    "        self.activations = output.detach()  # [N, C, H, W]\n",
    "\n",
    "    def remove(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        _ = self.model(x)   # forward pass\n",
    "        A = self.activations  # [N, C, H, W]\n",
    "        B, C, H, W = A.shape\n",
    "\n",
    "        # reshape to [C, H*W]\n",
    "        reshaped = A.reshape(C, -1).cpu().numpy()\n",
    "        reshaped -= reshaped.mean(axis=1, keepdims=True)\n",
    "\n",
    "        # PCA: first principal component\n",
    "        u, s, v = np.linalg.svd(reshaped, full_matrices=False)\n",
    "        cam = v[0].reshape(H, W)\n",
    "\n",
    "        # normalize [0,1]\n",
    "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
    "        return cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e1b15b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
