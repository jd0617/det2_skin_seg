{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76d71e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/dill-0.3.9-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.13a0+0d33366-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.11.9-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/igraph-0.11.8-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/texttable-1.7.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting grad-cam\n",
      "  Downloading grad-cam-1.5.5.tar.gz (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from grad-cam) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from grad-cam) (11.0.0)\n",
      "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.12/dist-packages (from grad-cam) (2.6.0a0+df5bbc09d1.nv24.12)\n",
      "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.12/dist-packages (from grad-cam) (0.20.0a0)\n",
      "Collecting ttach (from grad-cam)\n",
      "  Downloading ttach-0.0.3-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from grad-cam) (4.67.1)\n",
      "Collecting opencv-python (from grad-cam)\n",
      "  Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from grad-cam) (3.9.3)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from grad-cam) (1.5.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (70.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->grad-cam) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch>=1.7.1->grad-cam) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->grad-cam) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->grad-cam) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->grad-cam) (4.55.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->grad-cam) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->grad-cam) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->grad-cam) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->grad-cam) (2.9.0.post0)\n",
      "Collecting numpy (from grad-cam)\n",
      "  Downloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->grad-cam) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->grad-cam) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->grad-cam) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->grad-cam) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.7.1->grad-cam) (3.0.2)\n",
      "Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n",
      "Building wheels for collected packages: grad-cam\n",
      "  Building wheel for grad-cam (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for grad-cam: filename=grad_cam-1.5.5-py3-none-any.whl size=44284 sha256=e7b8886568d47efb834c932cf861d2c9aa445f1528ee187aa2c0080c4f92a08e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-t5gkmhvk/wheels/fb/3b/09/2afc520f3d69bc26ae6bd87416759c820a3f7d05c1a077bbf6\n",
      "Successfully built grad-cam\n",
      "Installing collected packages: ttach, numpy, opencv-python, grad-cam\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ultralytics 8.3.199 requires ultralytics-thop>=2.0.0, which is not installed.\n",
      "nvidia-modelopt 0.21.0 requires numpy<2, but you have numpy 2.2.6 which is incompatible.\n",
      "numba 0.59.1 requires numpy<1.27,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
      "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.2.6 which is incompatible.\n",
      "lightning-thunder 0.2.0.dev0 requires numpy<2,>=1.23.0, but you have numpy 2.2.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed grad-cam-1.5.5 numpy-2.2.6 opencv-python-4.12.0.88 ttach-0.0.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install grad-cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a4b33b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from detectron2.structures import Boxes, BoxMode\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self.hook_handles = []\n",
    "\n",
    "        # register hooks\n",
    "        self.hook_handles.append(target_layer.register_forward_hook(self._save_activation))\n",
    "        self.hook_handles.append(target_layer.register_backward_hook(self._save_gradient))\n",
    "\n",
    "    def _save_activation(self, module, input, output):\n",
    "        if output.ndim == 3:\n",
    "            output = output.unsqueeze(0)\n",
    "        self.activations = output.detach()\n",
    "\n",
    "    def _save_gradient(self, module, grad_input, grad_output):\n",
    "        grad = grad_output[0]\n",
    "        if grad.ndim == 3:\n",
    "            grad = grad.unsqueeze(0)\n",
    "        self.gradients = grad.detach()\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        for h in self.hook_handles:\n",
    "            h.remove()\n",
    "\n",
    "    def __call__(self, input_tensor, class_idx=None):\n",
    "        # Forward\n",
    "        output = self.model(input_tensor)\n",
    "        \n",
    "        # If it's a detection model, you'll need to pick a score or logit\n",
    "        if class_idx is None:\n",
    "            score = output[0][\"instances\"].scores[0]  # first detection\n",
    "        else:\n",
    "            # print(\"class_idx: \", class_idx)\n",
    "            # print(output[0])\n",
    "            score = output[0][\"instances\"].scores[int(class_idx)]\n",
    "\n",
    "        # Backward\n",
    "        self.model.zero_grad()\n",
    "        score.backward(retain_graph=True)\n",
    "\n",
    "        # Global-average-pool gradients\n",
    "        weights = self.gradients.mean(dim=[2, 3], keepdim=True)  # [C,1,1]\n",
    "\n",
    "        # Weighted sum of activations\n",
    "        cam = (weights * self.activations).sum(dim=1, keepdim=True)\n",
    "        cam = torch.relu(cam)\n",
    "        \n",
    "        # Normalize to [0,1]\n",
    "        cam = cam.squeeze().cpu().numpy()\n",
    "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
    "        return cam\n",
    "\n",
    "\n",
    "# ---- Visualizer helper ----\n",
    "def draw_predictions(img, outputs, metadata, gt_boxes, gt_classes):\n",
    "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
    "    inst = outputs[\"instances\"].to(\"cpu\")\n",
    "    # v = v.draw_instance_predictions(inst)\n",
    "\n",
    "    # if gt_boxes is not None:\n",
    "        # Draw GT boxes in red\n",
    "    for box, cls in zip(gt_boxes, gt_classes):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        cls_name = metadata.thing_classes[cls] if len(metadata.thing_classes) > 0 else str(cls)\n",
    "        text = f\"GT: {cls_name}\"\n",
    "\n",
    "        v.draw_box([x1, y1, x2, y2], edge_color=(0.0,1.0,0.0))\n",
    "        v.draw_text(text, (x1, y1), color=(0.0,1.0,0.0))\n",
    "\n",
    "    boxes = inst.pred_boxes.tensor.numpy()\n",
    "    scores = inst.scores.tolist()\n",
    "    classes = inst.pred_classes.tolist()\n",
    "\n",
    "    for box, score, cls_id in zip(boxes, scores, classes):\n",
    "        x0, y0, x1, y1 = box\n",
    "        cls_name = metadata.thing_classes[cls_id] if len(metadata.thing_classes) > 0 else str(cls_id)\n",
    "        text = f\"Pred: {cls_name} {score:.2f}\"\n",
    "\n",
    "        v.draw_box([x0, y0, x1, y1], edge_color=(1.0, 0.0, 0.0))\n",
    "        v.draw_text(text, (x0, y0), color=(1.0, 0.0, 0.0))\n",
    "\n",
    "    vis_out = v.output\n",
    "    drawn = vis_out.get_image()[:, :, ::-1]\n",
    "\n",
    "    return drawn\n",
    "\n",
    "\n",
    "def get_gt_from_dict(entry):\n",
    "    gt_boxes, gt_classes = [], []\n",
    "    for ann in entry[\"annotations\"]:\n",
    "        bbox = ann[\"bbox\"]\n",
    "        if ann[\"bbox_mode\"] != BoxMode.XYXY_ABS:\n",
    "            bbox = BoxMode.convert(bbox, ann[\"bbox_mode\"], BoxMode.XYXY_ABS)\n",
    "        gt_boxes.append(bbox)\n",
    "        gt_classes.append(ann[\"category_id\"])\n",
    "    return gt_boxes, gt_classes  \n",
    "\n",
    "\n",
    "# ---- Main routine for one image ----\n",
    "def visualize_cam_and_bboxes(entry, model, gradcam, metadata, out_dir):\n",
    "    img_path = entry[\"file_name\"]\n",
    "    h, w = entry[\"height\"], entry[\"width\"]\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    # Detectron2 input\n",
    "    inputs = [{\"image\": torch.as_tensor(img.astype(\"float32\").transpose(2, 0, 1)).cuda(),\n",
    "               \"height\": h, \"width\": w}]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "\n",
    "    # Grad-CAM on top detection (if any)\n",
    "    if len(outputs[0][\"instances\"]) > 0:\n",
    "        score = outputs[0][\"instances\"].scores[0]\n",
    "        cam_map = gradcam(inputs, score)\n",
    "        cam_resized = cv2.resize(cam_map, (w, h))\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * cam_resized), cv2.COLORMAP_JET)\n",
    "        overlay = (0.5 * heatmap + 0.5 * img).astype(np.uint8)\n",
    "    else:\n",
    "        overlay = img.copy()\n",
    "\n",
    "    # GT boxes\n",
    "    gt_boxes, gt_classes = get_gt_from_dict(entry)\n",
    "\n",
    "    # print(\"GT boxes:\", gt_boxes[:3])\n",
    "    # print(\"GT classes:\", gt_classes[:3])\n",
    "\n",
    "    # Pred + GT\n",
    "    bbox_vis = draw_predictions(img.copy(), outputs[0], metadata, gt_boxes, gt_classes)\n",
    "\n",
    "    # Stack horizontally\n",
    "    stacked = np.hstack([overlay, bbox_vis])\n",
    "\n",
    "    # Save\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    out_path = Path(out_dir) / Path(img_path).name\n",
    "    cv2.imwrite(str(out_path), stacked)\n",
    "    print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "078cca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import _init_paths\n",
    "from config import cfg, update_config\n",
    "\n",
    "def update_cfg_with_args(cfg, arg_key, arg_value):\n",
    "    cfg.defrost()\n",
    "\n",
    "    arg_key = arg_key.upper()\n",
    "\n",
    "    cfg.arg_key = arg_value\n",
    "\n",
    "    cfg.freeze()\n",
    "\n",
    "\n",
    "def minmax_norm(img):\n",
    "    minmax_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    img = np.array([\n",
    "        minmax_scaler.fit_transform(img[:, :, 0]),\n",
    "        minmax_scaler.fit_transform(img[:, :, 1]),\n",
    "        minmax_scaler.fit_transform(img[:, :, 2]),\n",
    "    ])\n",
    "\n",
    "    return np.transpose(img, (1, 2, 0)).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68815a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.data import DatasetCatalog\n",
    "\n",
    "from dataset.utils import register_patch_bin_dataset\n",
    "\n",
    "\n",
    "cfg_path = \"/workspace/project/configs/frcnn/frcnn.yaml\"\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\n",
    "        \"COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml\"\n",
    "))\n",
    "\n",
    "cfg.set_new_allowed(True)\n",
    "cfg.defrost()\n",
    "cfg.merge_from_file(cfg_path)\n",
    "\n",
    "cfg.MODEL.WEIGHTS = \"/workspace/project/record/debug2/result_single/frcnn_vis/model_final.pth\"\n",
    "cfg.MODEL.DEVICE = \"cuda\"\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.8   # e.g., 0.5, Removes duplicate boxes that overlap too much\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.9 # detection score threshold, Removes weak predictions\n",
    "\n",
    "cfg.freeze()\n",
    "\n",
    "predictor = DefaultPredictor(cfg)\n",
    "model = predictor.model\n",
    "model.eval()\n",
    "\n",
    "target_layer = model.backbone.bottom_up.res5[-1]\n",
    "gradcam = GradCAM(model, target_layer)\n",
    "\n",
    "register_patch_bin_dataset(\n",
    "        cfg.DATASETS.TEST[0],\n",
    "        json_file=cfg.DATASETS.TEST_ANNO_DIR,\n",
    "        img_root=cfg.DATASETS.IMG_DIR,\n",
    "        extra_key=[\"patient_id\"]\n",
    ")\n",
    "\n",
    "ds_dicts = DatasetCatalog.get(cfg.DATASETS.TEST[0])\n",
    "metadata = MetadataCatalog.get(cfg.DATASETS.TEST[0])\n",
    "\n",
    "out_dir = \"./gradcam_results/gradcam_results_80nms_90score\"\n",
    "\n",
    "for i, entry in enumerate(ds_dicts):\n",
    "        visualize_cam_and_bboxes(entry, model, gradcam, metadata, out_dir)\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027e59c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "img_path = \"/workspace/datasets/seg_by_patient/preprocessed/pos_cropped_patch_all_r1_r2/raw1_01-D2(30m)-1.JPG\"\n",
    "# Prepare image\n",
    "img = cv2.imread(img_path)[:, :, ::-1]  # BGR->RGB\n",
    "\n",
    "img = minmax_norm(img)\n",
    "\n",
    "# inputs = predictor.transform_gen.get_transform(img).apply_image(img)\n",
    "tensor = torch.as_tensor(img.transpose(2,0,1)).cuda().float()\n",
    "\n",
    "input = [{\n",
    "    \"image\":tensor,\n",
    "    \"height\": 1200,\n",
    "    \"width\": 600\n",
    "}]\n",
    "\n",
    "# print(tensor.shape)\n",
    "\n",
    "# Generate CAM\n",
    "cam_map = gradcam(input)\n",
    "\n",
    "# Overlay heatmap\n",
    "heatmap = cv2.applyColorMap(np.uint8(255 * cam_map), cv2.COLORMAP_JET)\n",
    "\n",
    "H, W, _ = img.shape\n",
    "\n",
    "r_hm = cv2.resize(heatmap, (W, H))\n",
    "\n",
    "r_hm = (r_hm-r_hm.min()) / (r_hm.max()-r_hm.min()+1e-8)\n",
    "\n",
    "heatmap = cv2.applyColorMap(np.uint8(255 * r_hm), cv2.COLORMAP_JET)\n",
    "\n",
    "\n",
    "overlay = 0.5 * heatmap[:, :, ::-1] + 0.5 * img\n",
    "cv2.imwrite(\"./gradcam_custom.jpg\", overlay[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34e1146",
   "metadata": {},
   "source": [
    "# EigenCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90cf12d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "\n",
    "# -------- EigenCAM --------\n",
    "class EigenCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.activations = None\n",
    "        self.hook = self.target_layer.register_forward_hook(self._hook_fn)\n",
    "\n",
    "    def _hook_fn(self, module, inputs, output):\n",
    "        self.activations = output.detach()  # [N, C, H, W]\n",
    "\n",
    "    def remove(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        _ = self.model(x)   # forward pass\n",
    "        A = self.activations  # [N, C, H, W]\n",
    "        B, C, H, W = A.shape\n",
    "\n",
    "        # reshape to [C, H*W]\n",
    "        reshaped = A.reshape(C, -1).cpu().numpy()\n",
    "        reshaped -= reshaped.mean(axis=1, keepdims=True)\n",
    "\n",
    "        # PCA: first principal component\n",
    "        u, s, v = np.linalg.svd(reshaped, full_matrices=False)\n",
    "        cam = v[0].reshape(H, W)\n",
    "\n",
    "        # normalize [0,1]\n",
    "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
    "        return cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77e1b15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config '/workspace/project/configs/frcnn/frcnn.yaml' has no VERSION. Assuming it to be compatible with latest v2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/fvcore/common/checkpoint.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw1_09-D2(30m)-4.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw1_09-D2(30m)-1.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw1_09-D2(30m)-2.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw1_09-D2(30m)-3.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw1_09-D3(24h)-3.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw1_09-D3(24h)-4.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw1_28-D2(30m)-4.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw1_28-D2(30m)-3.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw1_33-D2(30m)-1.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw1_33-D2(30m)-2.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw1_33-D2(30m)-3.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw1_33-D3(24h)-2.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw1_33-D2(30m)-4.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw1_33-D3(24h)-4.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_02-D2(30m)-4.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_02-D2(30m)-1.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_02-D2(30m)-2.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_02-D2(30m)-3.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_06-D2(30m)-1.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_06-D2(30m)-4.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_06-D2(30m)-2.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_06-D2(30m)-3.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_09-D2(30m)-4.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_09-D2(30m)-1.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_09-D2(30m)-2.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_09-D2(30m)-3.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_15-D2(30m)-3.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_15-(D3)24h-4.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_15-D2(30m)-1.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_15-D2(30m)-2.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_15-D2(30m)-4.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_17-(D3)24h-3.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_17-D2(30m)-4.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_17-(D3)24h-1.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_17-(D3)24h-4.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_17-D2(30m)-1.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_17-D2(30m)-2.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_17-D2(30m)-3.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_20-D2(30m)-4.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_20-D2(30m)-1.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_20-D2(30m)-2.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_20-D2(30m)-3.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_26-D2(30m)-3.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_26-D2(30m)-4.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_26-D2(30m)-1.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_26-D2(30m)-2.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_27-D2(30m)-3.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_27-D2(30m)-1.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_27-D2(30m)-2.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_27-D2(30m)-4.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_28-D2(30m)-4.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_28-D2(30m)-1.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_28-D2(30m)-2.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_28-D2(30m)-3.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_29-(D3)24h-4.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_29-D2(30m)-1.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_29-D2(30m)-2.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_29-D2(30m)-3.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_29-D2(30m)-4.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw1_02-D2(30m)-2.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw1_02-D2(30m)-3.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw1_02-D2(30m)-4.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw1_02-D3(24h)-1.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw1_02-D3(24h)-4.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_33-(D3)24h-4.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_33-D2(30m)-1.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_33-D2(30m)-2.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_33-D2(30m)-3.JPG\n",
      "Saved: cam_results/eigencam_results_80.0nms_50.0score/raw2_33-D2(30m)-4.JPG\n"
     ]
    }
   ],
   "source": [
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "\n",
    "from dataset.utils import register_patch_bin_dataset\n",
    "\n",
    "\n",
    "cfg_path = \"/workspace/project/configs/frcnn/frcnn.yaml\"\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\n",
    "        \"COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml\"\n",
    "))\n",
    "\n",
    "cfg.set_new_allowed(True)\n",
    "cfg.defrost()\n",
    "cfg.merge_from_file(cfg_path)\n",
    "\n",
    "cfg.MODEL.WEIGHTS = \"/workspace/project/record/debug22/result_bin/frcnn_th3/model_final.pth\"\n",
    "cfg.MODEL.DEVICE = \"cuda\"\n",
    "\n",
    "nms_th = 0.8\n",
    "score_th = 0.5\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = nms_th   # e.g., 0.5, Removes duplicate boxes that overlap too much\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = score_th # detection score threshold, Removes weak predictions\n",
    "\n",
    "cfg.freeze()\n",
    "\n",
    "predictor = DefaultPredictor(cfg)\n",
    "model = predictor.model\n",
    "model.eval()\n",
    "\n",
    "target_layer = model.backbone.bottom_up.res5[-1]\n",
    "gradcam = EigenCAM(model, target_layer)\n",
    "\n",
    "register_patch_bin_dataset(\n",
    "        cfg.DATASETS.TEST[0],\n",
    "        json_file=cfg.DATASETS.TEST_ANNO_DIR,\n",
    "        img_root=cfg.DATASETS.IMG_DIR,\n",
    "        extra_key=[\"patient_id\"]\n",
    ")\n",
    "\n",
    "ds_dicts = DatasetCatalog.get(cfg.DATASETS.TEST[0])\n",
    "metadata = MetadataCatalog.get(cfg.DATASETS.TEST[0])\n",
    "\n",
    "out_dir = f\"./cam_results/eigencam_results_{nms_th*100}nms_{score_th*100}score\"\n",
    "\n",
    "for i, entry in enumerate(ds_dicts):\n",
    "        visualize_cam_and_bboxes(entry, model, gradcam, metadata, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bc4694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config '/workspace/project/configs/frcnn/frcnn.yaml' has no VERSION. Assuming it to be compatible with latest v2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/fvcore/common/checkpoint.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [00:02<00:00, 31.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Results saved in eigencam_results/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "\n",
    "from pytorch_grad_cam import EigenCAM as eigencam\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "\n",
    "# --- Setup model ---\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml\"))\n",
    "\n",
    "cfg_path = \"/workspace/project/configs/frcnn/frcnn.yaml\"\n",
    "cfg.set_new_allowed(True)\n",
    "cfg.defrost()\n",
    "cfg.merge_from_file(cfg_path)\n",
    "\n",
    "cfg.MODEL.WEIGHTS = \"/workspace/project/record/debug22/result_bin/frcnn_th3/model_final.pth\"\n",
    "cfg.MODEL.DEVICE = \"cuda\"\n",
    "\n",
    "nms_th = 0.8\n",
    "score_th = 0.5\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = nms_th   # e.g., 0.5, Removes duplicate boxes that overlap too much\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = score_th # detection score threshold, Removes weak predictions\n",
    "\n",
    "cfg.freeze()\n",
    "\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# --- Pick a target layer (depends on backbone) ---\n",
    "target_layers = [predictor.model.backbone.bottom_up.res5[-1]]\n",
    "\n",
    "# --- Setup CAM ---\n",
    "cam = eigencam(model=predictor.model,\n",
    "               target_layers=target_layers)\n",
    "\n",
    "# --- Dataset (replace with your dataset name) ---\n",
    "register_patch_bin_dataset(\n",
    "        cfg.DATASETS.TEST[0],\n",
    "        json_file=cfg.DATASETS.TEST_ANNO_DIR,\n",
    "        img_root=cfg.DATASETS.IMG_DIR,\n",
    "        extra_key=[\"patient_id\"]\n",
    ")\n",
    "\n",
    "ds_dicts = DatasetCatalog.get(cfg.DATASETS.TEST[0])\n",
    "metadata = MetadataCatalog.get(cfg.DATASETS.TEST[0])\n",
    "\n",
    "# --- Output folder ---\n",
    "out_dir = f\"./cam_results/eigencam_results_{int(nms_th*100)}nms_{int(score_th*100)}score\"\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# --- Loop over dataset ---\n",
    "for d in tqdm(ds_dicts):\n",
    "    img_path = d[\"file_name\"]\n",
    "    image_bgr = cv2.imread(img_path)\n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "    image_rgb_float = np.float32(image_rgb) / 255.0\n",
    "\n",
    "    # Convert to tensor\n",
    "    input_tensor = torch.as_tensor(image_rgb_float.transpose(2,0,1)).unsqueeze(0).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Run detector\n",
    "    outputs = predictor(image_bgr)\n",
    "\n",
    "    # Skip empty detections\n",
    "    if len(outputs[\"instances\"]) == 0:\n",
    "        continue\n",
    "\n",
    "    # Run EigenCAM (EigenCAM ignores target class anyway)\n",
    "    grayscale_cam = cam(input_tensor=input_tensor, targets=None)\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "    # Overlay heatmap\n",
    "    visualization = show_cam_on_image(image_rgb_float, grayscale_cam, use_rgb=True)\n",
    "\n",
    "    # Draw bounding boxes too\n",
    "    instances = outputs[\"instances\"].to(\"cpu\")\n",
    "    for i in range(len(instances)):\n",
    "        box = instances.pred_boxes[i].tensor.numpy().astype(int)[0]\n",
    "        cls = metadata.thing_classes[instances.pred_classes[i]]\n",
    "        cv2.rectangle(visualization, (box[0], box[1]), (box[2], box[3]), (0,255,0), 2)\n",
    "        cv2.putText(visualization, cls, (box[0], box[1]-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 1)\n",
    "\n",
    "    # Save result\n",
    "    out_path = os.path.join(out_dir, \"eigencam_results\", os.path.basename(img_path))\n",
    "    cv2.imwrite(out_path, cv2.cvtColor(visualization, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "print(\"Done! Results saved in eigencam_results/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8dab06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
